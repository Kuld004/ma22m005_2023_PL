{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "9xXy6gO-CA4a"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "zADeY9L4Bqdd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task.1"
      ],
      "metadata": {
        "id": "0C95kCsJBzL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzRLrsgkBkB1",
        "outputId": "0eee8259-5f87-4f17-9aaf-5d2077af7702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    00006  1998  about  achieve  all  among  and  anything  are  aside  ...  \\\n",
            "0       0     0      0        0    1      0    0         0    0      0  ...   \n",
            "1       0     0      0        0    0      0    0         0    1      0  ...   \n",
            "2       0     0      0        1    1      0    2         0    0      0  ...   \n",
            "3       0     0      0        0    1      1    1         0    0      0  ...   \n",
            "4       0     0      0        0    0      0    0         0    0      0  ...   \n",
            "5       0     0      0        0    0      0    0         1    0      0  ...   \n",
            "6       0     1      0        0    0      0    0         0    0      0  ...   \n",
            "7       0     0      0        0    1      0    0         0    0      0  ...   \n",
            "8       1     0      1        0    0      0    0         0    0      0  ...   \n",
            "9       0     0      0        0    0      0    0         0    0      0  ...   \n",
            "10      0     0      0        0    0      0    0         0    0      0  ...   \n",
            "11      0     0      0        0    2      0    2         0    0      1  ...   \n",
            "12      0     0      0        0    1      0    2         0    0      0  ...   \n",
            "\n",
            "    we  were  wherefore  which  wikipedia  will  wilson1917  with  work  zero  \n",
            "0    0     0          0      0          0     0           0     2     0     0  \n",
            "1    1     0          0      0          0     0           1     0     1     0  \n",
            "2    0     0          0      1          0     0           0     0     0     0  \n",
            "3    0     0          0      0          0     0           0     1     0     0  \n",
            "4    0     0          0      0          0     0           0     0     0     1  \n",
            "5    0     0          0      0          0     1           0     0     0     0  \n",
            "6    0     0          0      0          1     0           0     0     0     0  \n",
            "7    0     0          0      0          0     0           0     0     0     0  \n",
            "8    0     0          0      0          0     0           0     0     0     0  \n",
            "9    0     1          0      0          0     0           0     0     0     0  \n",
            "10   0     0          0      0          0     0           0     0     0     0  \n",
            "11   0     0          1      0          0     0           0     0     0     0  \n",
            "12   0     0          0      0          0     0           0     0     0     0  \n",
            "\n",
            "[13 rows x 86 columns]\n"
          ]
        }
      ],
      "source": [
        "# Corpus from the exercise\n",
        "corpus = [\n",
        "    \"Lincoln1865: '...with malice toward none, with charity for all ...'\",\n",
        "    \"Wilson1917: '...It must strive to run to finish the work we are in ...'\",\n",
        "    \"'to do all which may achieve and cherish a just and lasting peace, '\",\n",
        "    \"'among ourselves, and with all nations.'\",\n",
        "    \"TrumpMay26: 'There is NO WAY (ZERO!) that Mail-In Ballots '\",\n",
        "    \"'will be anything less than substantially fraudulent.'\",\n",
        "    \"Wikipedia: 'In 1998, Oregon became the first state in the US '\",\n",
        "    \"'to conduct all voting exclusively by mail.'\",\n",
        "    \"FortuneMay26: 'Over the last two decades, about 0.00006% of total '\",\n",
        "    \"'vote-by-mail votes cast were fraudulent.'\",\n",
        "    \"TellAllPri07: 'Trump voted by mail in the Florida primary.'\",\n",
        "    \"KingJamesBible: 'Wherefore laying aside all malice, and all guile, and '\",\n",
        "    \"'hypocrisies, and envies, and all evil speakings,'\"\n",
        "]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Create the term-document matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Display the matrix as a data frame for better readability\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task.2"
      ],
      "metadata": {
        "id": "XqVD2hROB1xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the tokenizer function\n",
        "def spacy_tokenizer(document):\n",
        "    tokens = nlp(document)\n",
        "    return [token.lemma_ for token in tokens if not token.is_punct]\n",
        "\n",
        "# Initialize CountVectorizer with the spacy tokenizer\n",
        "vectorizer_spacy = CountVectorizer(tokenizer=spacy_tokenizer)\n",
        "\n",
        "# Create the term-document matrix\n",
        "X_spacy = vectorizer_spacy.fit_transform(corpus)\n",
        "\n",
        "# Display the matrix as a data frame for better readability\n",
        "df_spacy = pd.DataFrame(X_spacy.toarray(), columns=vectorizer_spacy.get_feature_names_out())\n",
        "\n",
        "print(df_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFwx_85WB3gx",
        "outputId": "27b9c5f7-cf6d-4c7b-c3b3-9ba5473e6bf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    0.00006  1998  a  about  achieve  all  among  and  anything  aside  ...  \\\n",
            "0         0     0  0      0        0    1      0    0         0      0  ...   \n",
            "1         0     0  0      0        0    0      0    0         0      0  ...   \n",
            "2         0     0  1      0        1    1      0    2         0      0  ...   \n",
            "3         0     0  0      0        0    1      1    1         0      0  ...   \n",
            "4         0     0  0      0        0    0      0    0         0      0  ...   \n",
            "5         0     0  0      0        0    0      0    0         1      0  ...   \n",
            "6         0     1  0      0        0    0      0    0         0      0  ...   \n",
            "7         0     0  0      0        0    1      0    0         0      0  ...   \n",
            "8         1     0  0      1        0    0      0    0         0      0  ...   \n",
            "9         0     0  0      0        0    0      0    0         0      0  ...   \n",
            "10        0     0  0      0        0    0      0    0         0      0  ...   \n",
            "11        0     0  0      0        0    2      0    2         0      1  ...   \n",
            "12        0     0  0      0        0    1      0    2         0      0  ...   \n",
            "\n",
            "    way  we  wherefore  which  wikipedia  will  wilson1917  with  work  zero  \n",
            "0     0   0          0      0          0     0           0     2     0     0  \n",
            "1     0   1          0      0          0     0           1     0     1     0  \n",
            "2     0   0          0      1          0     0           0     0     0     0  \n",
            "3     0   0          0      0          0     0           0     1     0     0  \n",
            "4     1   0          0      0          0     0           0     0     0     1  \n",
            "5     0   0          0      0          0     1           0     0     0     0  \n",
            "6     0   1          0      0          1     0           0     0     0     0  \n",
            "7     0   0          0      0          0     0           0     0     0     0  \n",
            "8     0   0          0      0          0     0           0     0     0     0  \n",
            "9     0   0          0      0          0     0           0     0     0     0  \n",
            "10    0   0          0      0          0     0           0     0     0     0  \n",
            "11    0   0          1      0          0     0           0     0     0     0  \n",
            "12    0   0          0      0          0     0           0     0     0     0  \n",
            "\n",
            "[13 rows x 81 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task.3"
      ],
      "metadata": {
        "id": "aObamgS6CM5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform LSA using TruncatedSVD\n",
        "lsa_model = TruncatedSVD(n_components=3)\n",
        "lsa_topic_matrix = lsa_model.fit_transform(X_spacy)\n",
        "\n",
        "# Display the three-dimensional LSA representation of the documents\n",
        "print(lsa_topic_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLgRAuhXCLi5",
        "outputId": "4b9e8393-9f01-4278-f962-2fa9a84c421d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.14123391 -0.37601529  0.30726399]\n",
            " [ 1.00679219  2.65079714 -0.77087247]\n",
            " [ 2.93162245 -0.47923397 -0.42757393]\n",
            " [ 1.43399476 -0.4368787   0.06409414]\n",
            " [ 0.32370801  1.27985684  1.26107408]\n",
            " [ 0.09229077  0.35483278  0.78481138]\n",
            " [ 0.6049887   2.86698755 -1.45847264]\n",
            " [ 0.97357338  0.4686548   0.97272987]\n",
            " [ 0.14931312  0.7518458  -0.7047208 ]\n",
            " [ 0.29554952  0.9511996   2.29279122]\n",
            " [ 0.44416077  1.71919247  1.24418365]\n",
            " [ 2.97395453 -0.9092797   0.03908033]\n",
            " [ 2.06781449 -0.62815435 -0.09649902]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task.4"
      ],
      "metadata": {
        "id": "NPVxSbrmCUNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute cosine similarity between two vectors\n",
        "def cosine_angle(v1, v2):\n",
        "    # Ensure the vectors are in the right shape (1, n_features)\n",
        "    v1 = v1.reshape(1, -1)\n",
        "    v2 = v2.reshape(1, -1)\n",
        "    return cosine_similarity(v1, v2)[0][0]\n",
        "\n",
        "# For the words, we need to use the components_ of the LSA model which represent the word vectors\n",
        "# in the topic space (the \"eigenwords\" if you will)\n",
        "word_vectors = lsa_model.components_\n",
        "\n",
        "# Get the vector for the word 'malice' and 'vote' from the word_vectors\n",
        "malice_vector = word_vectors[:, malice_idx].reshape(1, -1)\n",
        "vote_vector = word_vectors[:, vote_idx].reshape(1, -1)\n",
        "\n",
        "# Now we can compute the cosine similarity between the vectors of 'malice' and 'vote'\n",
        "cosine_sim = cosine_angle(malice_vector, vote_vector)\n",
        "print(f\"Cosine similarity between 'malice' and 'vote': {cosine_sim}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxAPTMCkC076",
        "outputId": "e34e0fe4-7525-4c3a-9373-3dfeff7d7062"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'malice' and 'vote': 0.11076024369466277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task.5"
      ],
      "metadata": {
        "id": "rnG8p5fMC73N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Term-document matrix is already computed and displayed in previous tasks.\n"
      ],
      "metadata": {
        "id": "6QETjkYwC9l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task .6\n"
      ],
      "metadata": {
        "id": "W4buKeUnDEoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The word vectors are in the components_ of the LSA model\n",
        "word_vectors_tfidf = lsa_tfidf_model.components_\n",
        "\n",
        "# Get the vector for the word 'malice' and 'vote' from the word_vectors using TF-IDF\n",
        "malice_vector_tfidf = word_vectors_tfidf[:, malice_tfidf_idx].reshape(1, -1)\n",
        "vote_vector_tfidf = word_vectors_tfidf[:, vote_tfidf_idx].reshape(1, -1)\n",
        "\n",
        "# Now we can compute the cosine similarity between the vectors of 'malice' and 'vote'\n",
        "cosine_sim_tfidf = cosine_angle(malice_vector_tfidf, vote_vector_tfidf)\n",
        "print(f\"Cosine similarity between 'malice' and 'vote' using TF-IDF: {cosine_sim_tfidf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh0Vpr_TDjXm",
        "outputId": "1350774c-e8a6-4790-a4f0-d7036f2fba71"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'malice' and 'vote' using TF-IDF: -0.026296222635368946\n"
          ]
        }
      ]
    }
  ]
}